{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45d9336-b376-4320-9316-e90173d8ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NIGERIA POST-HARVEST LOSSES ANALYSIS PROJECT SETUP\n",
      "================================================================================\n",
      "\n",
      "Setting up project structure...\n",
      "\n",
      "Created directory: data/raw\n",
      "Created directory: data/processed\n",
      "Created directory: data/final\n",
      "Created directory: scripts\n",
      "Created directory: visualizations\n",
      "Created directory: docs\n",
      "Created project configuration: project_config.json\n",
      "Created project README file: README.md\n",
      "\n",
      "Project structure setup complete!\n",
      "Next step: Run prepare_datasets.py to create clean, analysis-ready datasets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NIGERIA POST-HARVEST LOSSES ANALYSIS PROJECT SETUP\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSetting up project structure...\\n\")\n",
    "\n",
    "# Create directory structure\n",
    "directories = [\n",
    "    'data/raw',        # Original data files\n",
    "    'data/processed',  # Cleaned, validated data\n",
    "    'data/final',      # Analysis-ready datasets\n",
    "    'scripts',         # Analysis scripts\n",
    "    'visualizations',  # Output visualizations\n",
    "    'docs'             # Documentation\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created directory: {directory}\")\n",
    "\n",
    "# Create project configuration\n",
    "config = {\n",
    "    'project_name': 'Nigeria Post-Harvest Losses Analysis',\n",
    "    'start_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'primary_focus': 'Post-Harvest Losses',\n",
    "    'secondary_focus': 'Agricultural Finance',\n",
    "    'datasets': {\n",
    "        'post_harvest_losses': 'data/final/post_harvest_losses.csv',\n",
    "        'value_chain': 'data/final/value_chain.csv',\n",
    "        'financial_impact': 'data/final/financial_impact.csv',\n",
    "        'nutrient_losses': 'data/final/nutrient_losses.csv',\n",
    "        'climate_data': 'data/final/climate_data.csv'\n",
    "    },\n",
    "    'version': '1.0.0'\n",
    "}\n",
    "\n",
    "# Save project configuration\n",
    "with open('project_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "print(\"Created project configuration: project_config.json\")\n",
    "\n",
    "# Create README file with project information\n",
    "readme = \"\"\"# Nigeria Post-Harvest Losses Analysis Project\n",
    "\n",
    "## Project Overview\n",
    "This project analyzes post-harvest losses in Nigerian agriculture with a focus on agricultural finance.\n",
    "\n",
    "## Directory Structure\n",
    "- `data/raw`: Original data files\n",
    "- `data/processed`: Cleaned and validated data files\n",
    "- `data/final`: Analysis-ready datasets\n",
    "- `scripts`: Analysis and visualization scripts\n",
    "- `visualizations`: Output visualizations\n",
    "- `docs`: Project documentation\n",
    "\n",
    "## Datasets\n",
    "1. **Post-Harvest Losses**: Loss percentages by crop and region\n",
    "2. **Value Chain**: Losses at different stages of the agricultural value chain\n",
    "3. **Financial Impact**: Economic impact of post-harvest losses\n",
    "4. **Nutrient Losses**: Nutritional impact of post-harvest losses\n",
    "5. **Climate Data**: Climate data for contextual analysis\n",
    "\n",
    "## Analysis Workflow\n",
    "1. Data preparation using `prepare_datasets.py`\n",
    "2. Basic data visualization using `create_visualizations.py`\n",
    "3. Specialized analyses using specific scripts in the `scripts` directory\n",
    "\n",
    "## Getting Started\n",
    "1. Run `prepare_datasets.py` to set up the datasets\n",
    "2. Run `create_visualizations.py` to generate basic visualizations\n",
    "\"\"\"\n",
    "\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme)\n",
    "print(\"Created project README file: README.md\")\n",
    "\n",
    "print(\"\\nProject structure setup complete!\")\n",
    "print(\"Next step: Run prepare_datasets.py to create clean, analysis-ready datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df1a4a3-4361-46fa-906f-a449e4ad0865",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1467394783.py, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 140\u001b[1;36m\u001b[0m\n\u001b[1;33m    def process_post_harvest_losses(data/raw/post_narvest_loses.csv):\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NIGERIA POST-HARVEST LOSSES: DATA PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('data/original', exist_ok=True)\n",
    "os.makedirs('data/cleaned', exist_ok=True)\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# Function to detect and fix transposed data\n",
    "def fix_transposed_data(df, first_row_as_header=True, id_column=0):\n",
    "    \"\"\"\n",
    "    Fix datasets where the first row contains column headers\n",
    "    \"\"\"\n",
    "    # Check if the first row looks like headers\n",
    "    first_row = df.iloc[0].tolist()\n",
    "    has_header_keywords = any([\n",
    "        isinstance(x, str) and any(kw in str(x).lower() for kw in \n",
    "        ['maize', 'rice', 'crop', 'region', 'state', 'value', 'loss', '%', 'percentage', 'harvest', 'nutrient'])\n",
    "        for x in first_row\n",
    "    ])\n",
    "    \n",
    "    if first_row_as_header and has_header_keywords:\n",
    "        print(\"  - First row appears to contain headers - fixing transposed data\")\n",
    "        \n",
    "        # Extract headers from first row\n",
    "        headers = [str(x).strip() if not pd.isna(x) else f'Column_{i}' for i, x in enumerate(first_row)]\n",
    "        \n",
    "        # Replace empty or duplicate headers\n",
    "        seen = set()\n",
    "        for i, h in enumerate(headers):\n",
    "            if h in seen or h == '' or h == 'nan':\n",
    "                headers[i] = f'Column_{i}'\n",
    "            seen.add(headers[i])\n",
    "        \n",
    "        # Create new DataFrame with correct headers\n",
    "        fixed_df = pd.DataFrame(df.iloc[1:].values, columns=headers)\n",
    "        \n",
    "        # Convert string columns to numeric where possible\n",
    "        for col in fixed_df.columns:\n",
    "            if col != headers[id_column]:  # Skip the ID column\n",
    "                fixed_df[col] = pd.to_numeric(fixed_df[col], errors='coerce')\n",
    "        \n",
    "        return fixed_df\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to detect and clean data issues\n",
    "def clean_dataset(df, expected_structure=None):\n",
    "    \"\"\"\n",
    "    Perform general data cleaning:\n",
    "    1. Remove completely empty rows/columns\n",
    "    2. Convert numeric columns\n",
    "    3. Handle missing values\n",
    "    \"\"\"\n",
    "    print(\"  - Cleaning dataset\")\n",
    "    \n",
    "    # Remove completely empty rows and columns\n",
    "    df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "    \n",
    "    # Check for and convert non-numeric columns that should be numeric\n",
    "    for col in df.columns:\n",
    "        if col.lower() not in ['state', 'region', 'state/region', 'crop', 'crop_type', 'stage', 'nutrient', 'category', 'month', 'notes']:\n",
    "            # Try converting to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # If we have expectations about structure, ensure they're met\n",
    "    if expected_structure:\n",
    "        # Ensure all expected columns exist\n",
    "        for col in expected_structure:\n",
    "            if col not in df.columns:\n",
    "                print(f\"  - Warning: Expected column '{col}' not found\")\n",
    "    \n",
    "    # Fill or drop NaN values based on column type\n",
    "    for col in df.columns:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"  - Column '{col}' has {nan_count} missing values\")\n",
    "            \n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                # For numeric columns, fill with mean or 0\n",
    "                if df[col].count() > 0:  # If there are some non-NaN values\n",
    "                    mean_value = df[col].mean()\n",
    "                    print(f\"    - Filling missing values with mean: {mean_value}\")\n",
    "                    df[col] = df[col].fillna(mean_value)\n",
    "                else:\n",
    "                    print(f\"    - Filling missing values with 0\")\n",
    "                    df[col] = df[col].fillna(0)\n",
    "            else:\n",
    "                # For categorical/text columns, fill with 'Unknown' or most common value\n",
    "                if df[col].count() > 0:\n",
    "                    most_common = df[col].value_counts().index[0]\n",
    "                    print(f\"    - Filling missing values with most common value: '{most_common}'\")\n",
    "                    df[col] = df[col].fillna(most_common)\n",
    "                else:\n",
    "                    print(f\"    - Filling missing values with 'Unknown'\")\n",
    "                    df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to reshape wide format to long format\n",
    "def reshape_to_long(df, id_vars, var_name, value_name):\n",
    "    \"\"\"\n",
    "    Reshape data from wide to long format\n",
    "    \"\"\"\n",
    "    print(f\"  - Reshaping to long format with {id_vars} as ID columns\")\n",
    "    \n",
    "    # Ensure id_vars are actually in the DataFrame\n",
    "    valid_id_vars = [col for col in id_vars if col in df.columns]\n",
    "    if not valid_id_vars:\n",
    "        print(f\"  - Warning: None of the specified ID columns {id_vars} found in DataFrame\")\n",
    "        # Use the first column as ID\n",
    "        valid_id_vars = [df.columns[0]]\n",
    "        print(f\"  - Using {valid_id_vars} as ID column instead\")\n",
    "    \n",
    "    # All columns except ID columns will be \"melted\"\n",
    "    value_vars = [col for col in df.columns if col not in valid_id_vars]\n",
    "    \n",
    "    # Perform the melt operation\n",
    "    long_df = df.melt(\n",
    "        id_vars=valid_id_vars,\n",
    "        value_vars=value_vars,\n",
    "        var_name=var_name,\n",
    "        value_name=value_name\n",
    "    )\n",
    "    \n",
    "    # Drop rows where the value is NaN\n",
    "    long_df = long_df.dropna(subset=[value_name])\n",
    "    \n",
    "    return long_df\n",
    "\n",
    "# Function to process post-harvest losses data\n",
    "def process_post_harvest_losses(data/raw/post_narvest_loses.csv):\n",
    "    \"\"\"\n",
    "    Process post-harvest losses dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing post-harvest losses data from: {file_path}\")\n",
    "    \n",
    "    # Read the file with flexible parser\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"  - Successfully read with comma separator\")\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"  - Successfully read with semicolon separator\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "                print(f\"  - Successfully read with auto-detected separator\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading file: {str(e)}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"  - Original shape: {df.shape}\")\n",
    "    \n",
    "    # Check if data is transposed (crop names in first row)\n",
    "    df = fix_transposed_data(df)\n",
    "    \n",
    "    # Clean the dataset\n",
    "    df = clean_dataset(df)\n",
    "    \n",
    "    # Identify key columns\n",
    "    # Look for a column that might be states/regions\n",
    "    region_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['state', 'region', 'state/region', 'location', 'area']:\n",
    "            region_col = col\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        # If no clear region column, use the first column\n",
    "        region_col = df.columns[0]\n",
    "        print(f\"  - Using '{region_col}' as the region column\")\n",
    "    \n",
    "    # If we have crop columns in wide format, reshape to long format\n",
    "    if len(df.columns) > 3 and all(col not in ['stage', 'value chain'] for col in df.columns):\n",
    "        df = reshape_to_long(\n",
    "            df=df, \n",
    "            id_vars=[region_col], \n",
    "            var_name='crop_type', \n",
    "            value_name='loss_percentage'\n",
    "        )\n",
    "    \n",
    "    print(f\"  - Final shape: {df.shape}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    output_file = 'data/cleaned/post_harvest_losses_cleaned.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  - Saved cleaned dataset to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to process value chain data\n",
    "def process_value_chain(data/raw/value chain.csv):\n",
    "    \"\"\"\n",
    "    Process value chain dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing value chain data from: {file_path}\")\n",
    "    \n",
    "    # Check if file exists and has content\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "        print(f\"  - File is empty or doesn't exist: {file_path}\")\n",
    "        print(f\"  - Creating synthetic value chain dataset\")\n",
    "        \n",
    "        # Create example data\n",
    "        data = {\n",
    "            'crop_type': ['Maize', 'Maize', 'Maize', 'Maize', 'Maize', \n",
    "                         'Rice', 'Rice', 'Rice', 'Rice', 'Rice',\n",
    "                         'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum'],\n",
    "            'stage': ['Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                     'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                     'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing'],\n",
    "            'loss_percentage': [6.42, 4.0, 1.32, 2.37, 4.71,\n",
    "                               5.12, 3.8, 2.11, 1.98, 3.45,\n",
    "                               4.89, 2.76, 3.21, 1.55, 2.98]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        # Read the file with flexible parser\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Successfully read with comma separator\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=';')\n",
    "                print(f\"  - Successfully read with semicolon separator\")\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "                    print(f\"  - Successfully read with auto-detected separator\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Error reading file: {str(e)}\")\n",
    "                    return None\n",
    "    \n",
    "    print(f\"  - Original shape: {df.shape}\")\n",
    "    \n",
    "    # Check if data is empty\n",
    "    if df.empty or df.shape[0] <= 1:\n",
    "        print(f\"  - Dataset is empty or has only headers\")\n",
    "        print(f\"  - Creating synthetic value chain dataset\")\n",
    "        \n",
    "        # Create example data\n",
    "        data = {\n",
    "            'crop_type': ['Maize', 'Maize', 'Maize', 'Maize', 'Maize', \n",
    "                         'Rice', 'Rice', 'Rice', 'Rice', 'Rice',\n",
    "                         'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum'],\n",
    "            'stage': ['Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                     'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                     'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing'],\n",
    "            'loss_percentage': [6.42, 4.0, 1.32, 2.37, 4.71,\n",
    "                               5.12, 3.8, 2.11, 1.98, 3.45,\n",
    "                               4.89, 2.76, 3.21, 1.55, 2.98]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        # Check if data is transposed (stage names in first row)\n",
    "        df = fix_transposed_data(df)\n",
    "        \n",
    "        # Clean the dataset\n",
    "        df = clean_dataset(df)\n",
    "        \n",
    "        # Check if we have a wide format (crops as rows, stages as columns)\n",
    "        stage_keywords = ['harvest', 'dry', 'store', 'transport', 'process', 'market', 'retail']\n",
    "        has_stage_columns = any(any(keyword in col.lower() for keyword in stage_keywords) \n",
    "                              for col in df.columns if isinstance(col, str))\n",
    "        \n",
    "        if has_stage_columns:\n",
    "            print(\"  - Dataset appears to be in wide format (stages as columns)\")\n",
    "            \n",
    "            # Get the crop column (usually first column)\n",
    "            crop_col = df.columns[0]\n",
    "            \n",
    "            # Get the stage columns\n",
    "            stage_cols = [col for col in df.columns if col != crop_col]\n",
    "            \n",
    "            # Reshape to long format\n",
    "            df = df.melt(\n",
    "                id_vars=[crop_col],\n",
    "                value_vars=stage_cols,\n",
    "                var_name='stage',\n",
    "                value_name='loss_percentage'\n",
    "            )\n",
    "            \n",
    "            # Rename crop column if needed\n",
    "            if crop_col.lower() != 'crop_type':\n",
    "                df = df.rename(columns={crop_col: 'crop_type'})\n",
    "        \n",
    "        # Ensure we have the required columns\n",
    "        required_cols = ['crop_type', 'stage', 'loss_percentage']\n",
    "        \n",
    "        # Check if we're missing any required columns\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"  - Warning: Missing required columns: {missing_cols}\")\n",
    "            print(f\"  - Creating synthetic value chain dataset\")\n",
    "            \n",
    "            # Create example data\n",
    "            data = {\n",
    "                'crop_type': ['Maize', 'Maize', 'Maize', 'Maize', 'Maize', \n",
    "                             'Rice', 'Rice', 'Rice', 'Rice', 'Rice',\n",
    "                             'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum', 'Sorghum'],\n",
    "                'stage': ['Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                         'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing',\n",
    "                         'Harvesting', 'Drying', 'Storage', 'Transportation', 'Processing'],\n",
    "                'loss_percentage': [6.42, 4.0, 1.32, 2.37, 4.71,\n",
    "                                   5.12, 3.8, 2.11, 1.98, 3.45,\n",
    "                                   4.89, 2.76, 3.21, 1.55, 2.98]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert loss_percentage to numeric\n",
    "    df['loss_percentage'] = pd.to_numeric(df['loss_percentage'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing loss_percentage\n",
    "    df = df.dropna(subset=['loss_percentage'])\n",
    "    \n",
    "    print(f\"  - Final shape: {df.shape}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    output_file = 'data/cleaned/value_chain_cleaned.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  - Saved cleaned dataset to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to process financial impact data\n",
    "def process_financial_data(data/raw/financial-impact.csv):\n",
    "    \"\"\"\n",
    "    Process financial impact dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing financial impact data from: {file_path}\")\n",
    "    \n",
    "    # Read the file with flexible parser\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"  - Successfully read with comma separator\")\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"  - Successfully read with semicolon separator\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "                print(f\"  - Successfully read with auto-detected separator\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading file: {str(e)}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"  - Original shape: {df.shape}\")\n",
    "    \n",
    "    # Check if data is transposed (has headers in first row)\n",
    "    df = fix_transposed_data(df)\n",
    "    \n",
    "    # Clean the dataset\n",
    "    df = clean_dataset(df)\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['crop_type', 'financial_value']\n",
    "    missing_cols = [col for col in required_cols if not any(c.lower() == col.lower() for c in df.columns)]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"  - Warning: Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Try to identify crop column\n",
    "        crop_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() in ['crop', 'crop_type', 'crop type', 'commodity', 'product']:\n",
    "                crop_col = col\n",
    "                break\n",
    "        \n",
    "        if crop_col is None:\n",
    "            crop_col = df.columns[0]\n",
    "            print(f\"  - Using '{crop_col}' as the crop column\")\n",
    "        \n",
    "        # Try to identify financial value column\n",
    "        value_col = None\n",
    "        for col in df.columns:\n",
    "            if any(kw in col.lower() for kw in ['value', 'financial', 'loss', 'impact', 'cost', 'usd', '$', 'naira']):\n",
    "                value_col = col\n",
    "                break\n",
    "        \n",
    "        if value_col is None:\n",
    "            # Look for a column with numeric values\n",
    "            for col in df.columns:\n",
    "                if col != crop_col and df[col].dtype in ['int64', 'float64']:\n",
    "                    value_col = col\n",
    "                    break\n",
    "        \n",
    "        if value_col is not None:\n",
    "            print(f\"  - Using '{value_col}' as the financial value column\")\n",
    "            \n",
    "            # Rename columns to standard names\n",
    "            df = df.rename(columns={crop_col: 'crop_type', value_col: 'financial_value'})\n",
    "        else:\n",
    "            print(f\"  - Could not identify financial value column\")\n",
    "            print(f\"  - Creating synthetic financial data\")\n",
    "            \n",
    "            # Create example data\n",
    "            data = {\n",
    "                'crop_type': ['Maize', 'Rice', 'Sorghum', 'Millet', 'Cassava'],\n",
    "                'financial_value': [1248197000, 978456000, 567123000, 345678000, 789012000],\n",
    "                'region': ['National', 'National', 'National', 'National', 'National']\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert financial_value to numeric\n",
    "    df['financial_value'] = pd.to_numeric(df['financial_value'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing financial_value\n",
    "    df = df.dropna(subset=['financial_value'])\n",
    "    \n",
    "    # Ensure we have a region column\n",
    "    if 'region' not in df.columns:\n",
    "        df['region'] = 'National'\n",
    "    \n",
    "    print(f\"  - Final shape: {df.shape}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    output_file = 'data/cleaned/financial_impact_cleaned.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  - Saved cleaned dataset to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to process nutrient losses data\n",
    "def process_nutrient_data(file_path):\n",
    "    \"\"\"\n",
    "    Process nutrient losses dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing nutrient losses data from: {file_path}\")\n",
    "    \n",
    "    # Read the file with flexible parser\n",
    "    try:\n",
    "        df = pd.read_csv(data/raw/nutrint_losses.csv)\n",
    "        print(f\"  - Successfully read with comma separator\")\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"  - Successfully read with semicolon separator\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "                print(f\"  - Successfully read with auto-detected separator\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading file: {str(e)}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"  - Original shape: {df.shape}\")\n",
    "    \n",
    "    # Check if data is transposed (crop names in first row)\n",
    "    df = fix_transposed_data(df)\n",
    "    \n",
    "    # Clean the dataset\n",
    "    df = clean_dataset(df)\n",
    "    \n",
    "    # Check if we have nutrient names in the first column\n",
    "    nutrient_col = df.columns[0]\n",
    "    if nutrient_col.lower() in ['nutrient', 'energy', 'nutrient type', 'nutrition']:\n",
    "        print(f\"  - Nutrient names found in column '{nutrient_col}'\")\n",
    "        \n",
    "        # Reshape to long format\n",
    "        crop_cols = [col for col in df.columns if col != nutrient_col]\n",
    "        \n",
    "        df = df.melt(\n",
    "            id_vars=[nutrient_col],\n",
    "            value_vars=crop_cols,\n",
    "            var_name='crop_type',\n",
    "            value_name='nutrient_loss'\n",
    "        )\n",
    "        \n",
    "        # Rename nutrient column if needed\n",
    "        if nutrient_col.lower() != 'nutrient':\n",
    "            df = df.rename(columns={nutrient_col: 'nutrient'})\n",
    "    else:\n",
    "        print(f\"  - Could not identify nutrient column\")\n",
    "        print(f\"  - Creating synthetic nutrient data\")\n",
    "        \n",
    "        # Create example data\n",
    "        nutrients = ['Energy (kcal)', 'Protein (g)', 'Fat (g)', 'Carbohydrate (g)', 'Fiber (g)', 'Vitamin A (Î¼g)']\n",
    "        crops = ['Maize', 'Rice', 'Sorghum', 'Millet']\n",
    "        \n",
    "        data = []\n",
    "        for nutrient in nutrients:\n",
    "            for crop in crops:\n",
    "                # Generate a random value based on the nutrient type\n",
    "                if 'Energy' in nutrient:\n",
    "                    value = np.random.uniform(2000000000, 8000000000)\n",
    "                elif 'Protein' in nutrient:\n",
    "                    value = np.random.uniform(100000, 500000)\n",
    "                elif 'Fat' in nutrient:\n",
    "                    value = np.random.uniform(50000, 200000)\n",
    "                elif 'Carbohydrate' in nutrient:\n",
    "                    value = np.random.uniform(500000, 1500000)\n",
    "                elif 'Fiber' in nutrient:\n",
    "                    value = np.random.uniform(20000, 100000)\n",
    "                else:\n",
    "                    value = np.random.uniform(10000, 50000)\n",
    "                \n",
    "                data.append({\n",
    "                    'nutrient': nutrient,\n",
    "                    'crop_type': crop,\n",
    "                    'nutrient_loss': value\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert nutrient_loss to numeric\n",
    "    df['nutrient_loss'] = pd.to_numeric(df['nutrient_loss'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing nutrient_loss\n",
    "    df = df.dropna(subset=['nutrient_loss'])\n",
    "    \n",
    "    print(f\"  - Final shape: {df.shape}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    output_file = 'data/cleaned/nutrient_losses_cleaned.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  - Saved cleaned dataset to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to process climate data\n",
    "def process_climate_data(data/raw/Nigeria_climate_change.csv):\n",
    "    \"\"\"\n",
    "    Process climate dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing climate data from: {file_path}\")\n",
    "    \n",
    "    # Read the file with flexible parser\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"  - Successfully read with comma separator\")\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';')\n",
    "            print(f\"  - Successfully read with semicolon separator\")\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "                print(f\"  - Successfully read with auto-detected separator\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error reading file: {str(e)}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"  - Original shape: {df.shape}\")\n",
    "    \n",
    "    # Clean the dataset\n",
    "    df = clean_dataset(df)\n",
    "    \n",
    "    # Check if we have expected columns\n",
    "    expected_cols = ['category', 'temperature', 'precipitation']\n",
    "    expected_cols_found = [col for col in expected_cols if any(c.lower() == col.lower() or col.lower() in c.lower() for c in df.columns)]\n",
    "    \n",
    "    if len(expected_cols_found) < 2:\n",
    "        print(f\"  - Warning: Expected climate data columns not found\")\n",
    "        \n",
    "        # Try to identify month/category column\n",
    "        month_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() in ['month', 'category', 'period', 'time', 'date']:\n",
    "                month_col = col\n",
    "                break\n",
    "        \n",
    "        if month_col is None:\n",
    "            # If first column has month names, use it\n",
    "            if df.iloc[:, 0].str.lower().str.contains('jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec').any():\n",
    "                month_col = df.columns[0]\n",
    "        \n",
    "        if month_col:\n",
    "            print(f\"  - Found month/category column: '{month_col}'\")\n",
    "            \n",
    "            # Rename to standard name\n",
    "            df = df.rename(columns={month_col: 'Category'})\n",
    "        else:\n",
    "            print(f\"  - Could not identify month/category column\")\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in df.columns:\n",
    "        if col != 'Category':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  - Final shape: {df.shape}\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    output_file = 'data/cleaned/climate_data_cleaned.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  - Saved cleaned dataset to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to find data files in current directory\n",
    "def find_data_files():\n",
    "    \"\"\"\n",
    "    Find and classify data files in the current directory\n",
    "    \"\"\"\n",
    "    print(\"\\nSearching for data files...\")\n",
    "    \n",
    "    datasets = {\n",
    "        'post_harvest_losses': None,\n",
    "        'value_chain': None,\n",
    "        'financial_impact': None,\n",
    "        'nutrient_losses': None,\n",
    "        'climate_data': None\n",
    "    }\n",
    "    \n",
    "    # Look for files in data directories\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_lower = file.lower()\n",
    "                \n",
    "                if 'harvest' in file_lower and 'loss' in file_lower:\n",
    "                    datasets['post_harvest_losses'] = file_path\n",
    "                    print(f\"Found post-harvest losses data: {file_path}\")\n",
    "                \n",
    "                elif 'value' in file_lower and 'chain' in file_lower:\n",
    "                    datasets['value_chain'] = file_path\n",
    "                    print(f\"Found value chain data: {file_path}\")\n",
    "                \n",
    "                elif ('financ' in file_lower or 'economic' in file_lower) and ('impact' in file_lower or 'loss' in file_lower):\n",
    "                    datasets['financial_impact'] = file_path\n",
    "                    print(f\"Found financial impact data: {file_path}\")\n",
    "                \n",
    "                elif 'nutri' in file_lower and 'loss' in file_lower:\n",
    "                    datasets['nutrient_losses'] = file_path\n",
    "                    print(f\"Found nutrient losses data: {file_path}\")\n",
    "                \n",
    "                elif 'climate' in file_lower or 'weather' in file_lower:\n",
    "                    datasets['climate_data'] = file_path\n",
    "                    print(f\"Found climate data: {file_path}\")\n",
    "    \n",
    "    # Check for missing datasets\n",
    "    missing = [k for k, v in datasets.items() if v is None]\n",
    "    if missing:\n",
    "        print(f\"Warning: Could not find data files for: {', '.join(missing)}\")\n",
    "        \n",
    "        # Look for any CSV files\n",
    "        csv_files = []\n",
    "        for root, dirs, files in os.walk('.'):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_files.append(os.path.join(root, file))\n",
    "        \n",
    "        if csv_files:\n",
    "            print(f\"Found {len(csv_files)} CSV files:\")\n",
    "            for i, file in enumerate(csv_files):\n",
    "                print(f\"  {i+1}. {file}\")\n",
    "            \n",
    "            print(\"\\nBased on the available CSV files, making best guesses:\")\n",
    "            \n",
    "            # Try to assign unidentified files to missing datasets\n",
    "            for dataset_type in missing:\n",
    "                if csv_files:\n",
    "                    datasets[dataset_type] = csv_files.pop(0)\n",
    "                    print(f\"  Assigned {datasets[dataset_type]} to {dataset_type}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Create project configuration\n",
    "    project_config = {\n",
    "        'project_name': 'Nigeria Post-Harvest Losses Analysis',\n",
    "        'primary_focus': 'Post-Harvest Losses',\n",
    "        'secondary_focus': 'Agricultural Finance',\n",
    "        'start_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'datasets': {}\n",
    "    }\n",
    "    \n",
    "    # Find data files\n",
    "    datasets = find_data_files()\n",
    "    \n",
    "    # Process each dataset type\n",
    "    processed_datasets = {}\n",
    "    \n",
    "    # Process post-harvest losses data\n",
    "    if datasets['post_harvest_losses']:\n",
    "        df = process_post_harvest_losses(datasets['post_harvest_losses'])\n",
    "        if df is not None:\n",
    "            processed_datasets['post_harvest_losses'] = 'data/cleaned/post_harvest_losses_cleaned.csv'\n",
    "            # Backup original data\n",
    "            shutil.copy2(datasets['post_harvest_losses'], f'data/original/{os.path.basename(datasets[\"post_harvest_losses\"])}')\n",
    "    else:\n",
    "        print(\"\\nPost-harvest losses data file not found. Skipping processing.\")\n",
    "    \n",
    "    # Process value chain data\n",
    "    if datasets['value_chain']:\n",
    "        df = process_value_chain(datasets['value_chain'])\n",
    "        if df is not None:\n",
    "            processed_datasets['value_chain'] = 'data/cleaned/value_chain_cleaned.csv'\n",
    "            # Backup original data\n",
    "            shutil.copy2(datasets['value_chain'], f'data/original/{os.path.basename(datasets[\"value_chain\"])}')\n",
    "    else:\n",
    "        print(\"\\nValue chain data file not found. Creating synthetic data.\")\n",
    "        df = process_value_chain(None)\n",
    "        if df is not None:\n",
    "            processed_datasets['value_chain'] = 'data/cleaned/value_chain_cleaned.csv'\n",
    "    \n",
    "    # Process financial impact data\n",
    "    if datasets['financial_impact']:\n",
    "        df = process_financial_data(datasets['financial_impact'])\n",
    "        if df is not None:\n",
    "            processed_datasets['financial_impact'] = 'data/cleaned/financial_impact_cleaned.csv'\n",
    "            # Backup original data\n",
    "            shutil.copy2(datasets['financial_impact'], f'data/original/{os.path.basename(datasets[\"financial_impact\"])}')\n",
    "    else:\n",
    "        print(\"\\nFinancial impact data file not found. Skipping processing.\")\n",
    "    \n",
    "    # Process nutrient losses data\n",
    "    if datasets['nutrient_losses']:\n",
    "        df = process_nutrient_data(datasets['nutrient_losses'])\n",
    "        if df is not None:\n",
    "            processed_datasets['nutrient_losses'] = 'data/cleaned/nutrient_losses_cleaned.csv'\n",
    "            # Backup original data\n",
    "            shutil.copy2(datasets['nutrient_losses'], f'data/original/{os.path.basename(datasets[\"nutrient_losses\"])}')\n",
    "    else:\n",
    "        print(\"\\nNutrient losses data file not found. Skipping processing.\")\n",
    "    \n",
    "    # Process climate data\n",
    "    if datasets['climate_data']:\n",
    "        df = process_climate_data(datasets['climate_data'])\n",
    "        if df is not None:\n",
    "            processed_datasets['climate_data'] = 'data/cleaned/climate_data_cleaned.csv'\n",
    "            # Backup original data\n",
    "            shutil.copy2(datasets['climate_data'], f'data/original/{os.path.basename(datasets[\"climate_data\"])}')\n",
    "    else:\n",
    "        print(\"\\nClimate data file not found. Skipping processing.\")\n",
    "    \n",
    "    # Update project configuration with processed datasets\n",
    "    project_config['datasets'] = processed_datasets\n",
    "    \n",
    "    # Save project configuration\n",
    "    with open('project_config.json', 'w') as f:\n",
    "        json.dump(project_config, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA PREPARATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Processed Datasets: {len(processed_datasets)}\")\n",
    "    for dataset_type, file_path in processed_datasets.items():\n",
    "        print(f\"  - {dataset_type}: {file_path}\")\n",
    "    print(\"\\nProject configuration saved to: project_config.json\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Run the visualizations.py script to create plots\")\n",
    "    print(\"  2. Check the results/plots directory for generated visualizations\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a830c4c2-0a7c-418d-a397-bca7594c2efd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 670) (446974296.py, line 670)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 670\u001b[1;36m\u001b[0m\n\u001b[1;33m    fin_crops = set(datasets['financial_impact']['crop\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 670)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NIGERIA POST-HARVEST LOSSES: VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "\n",
    "# Load project configuration\n",
    "try:\n",
    "    with open('project_config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Project configuration loaded successfully\")\n",
    "    print(f\"Project: {config['project_name']}\")\n",
    "    print(f\"Primary focus: {config['primary_focus']}\")\n",
    "    print(f\"Secondary focus: {config['secondary_focus']}\")\n",
    "    print(f\"Start date: {config['start_date']}\")\n",
    "    print(f\"Datasets: {len(config['datasets'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading project configuration: {str(e)}\")\n",
    "    config = {'datasets': {}}\n",
    "\n",
    "# Function to format large numbers\n",
    "def format_large_number(value):\n",
    "    if pd.isna(value):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    if value >= 1e9:\n",
    "        return f\"{value/1e9:.1f}B\"\n",
    "    elif value >= 1e6:\n",
    "        return f\"{value/1e6:.1f}M\"\n",
    "    elif value >= 1e3:\n",
    "        return f\"{value/1e3:.1f}K\"\n",
    "    else:\n",
    "        return f\"{value:.1f}\"\n",
    "\n",
    "# Function to create post-harvest losses visualizations\n",
    "def visualize_post_harvest_losses(file_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for post-harvest losses data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating post-harvest losses visualizations...\")\n",
    "    \n",
    "    if file_path is None:\n",
    "        file_path = config['datasets'].get('post_harvest_losses')\n",
    "    \n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"  - Post-harvest losses data file not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded data shape: {df.shape}\")\n",
    "    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'crop_type' not in df.columns or 'loss_percentage' not in df.columns:\n",
    "        print(\"  - Error: Data does not have the expected columns (crop_type, loss_percentage)\")\n",
    "        return\n",
    "    \n",
    "    # 1. Create a bar chart of post-harvest losses by crop type\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate average loss by crop type\n",
    "    crop_losses = df.groupby('crop_type')['loss_percentage'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    # Create the plot\n",
    "    sns.barplot(x=crop_losses.index, y=crop_losses.values)\n",
    "    plt.title('Average Post-Harvest Losses by Crop Type in Nigeria', fontsize=16)\n",
    "    plt.xlabel('Crop Type', fontsize=14)\n",
    "    plt.ylabel('Loss Percentage (%)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(crop_losses.values):\n",
    "        plt.text(i, v + 0.5, f'{v:.1f}%', ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/post_harvest_losses_by_crop.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 2. Create a heatmap of post-harvest losses by crop type and region (if region data available)\n",
    "    if 'State/Region' in df.columns:\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # Create a pivot table\n",
    "        pivot = df.pivot_table(\n",
    "            values='loss_percentage',\n",
    "            index='State/Region',\n",
    "            columns='crop_type',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Limit to top 15 regions by total loss\n",
    "        if len(pivot) > 15:\n",
    "            pivot = pivot.loc[pivot.sum(axis=1).nlargest(15).index]\n",
    "        \n",
    "        # Create the heatmap\n",
    "        sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=0.5)\n",
    "        plt.title('Post-Harvest Losses by Crop Type and Region in Nigeria (%)', fontsize=16)\n",
    "        plt.xlabel('Crop Type', fontsize=14)\n",
    "        plt.ylabel('Region', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_file = 'results/plots/post_harvest_losses_heatmap.png'\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    print(\"  - Post-harvest losses visualizations created successfully\")\n",
    "\n",
    "# Function to create value chain visualizations\n",
    "def visualize_value_chain(file_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for value chain data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating value chain visualizations...\")\n",
    "    \n",
    "    if file_path is None:\n",
    "        file_path = config['datasets'].get('value_chain')\n",
    "    \n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"  - Value chain data file not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded data shape: {df.shape}\")\n",
    "    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'crop_type' not in df.columns or 'stage' not in df.columns or 'loss_percentage' not in df.columns:\n",
    "        print(\"  - Error: Data does not have the expected columns (crop_type, stage, loss_percentage)\")\n",
    "        return\n",
    "    \n",
    "    # 1. Create a grouped bar chart of losses by stage and crop type\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create the plot\n",
    "    ax = sns.barplot(x='stage', y='loss_percentage', hue='crop_type', data=df)\n",
    "    plt.title('Post-Harvest Losses by Value Chain Stage and Crop Type', fontsize=16)\n",
    "    plt.xlabel('Value Chain Stage', fontsize=14)\n",
    "    plt.ylabel('Loss Percentage (%)', fontsize=14)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(title='Crop Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/value_chain_losses_by_stage.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 2. Create a stacked area chart showing cumulative losses across the value chain\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get list of unique crops and stages\n",
    "    crops = sorted(df['crop_type'].unique())\n",
    "    stages = sorted(df['stage'].unique(), key=lambda x: df[df['stage'] == x]['loss_percentage'].mean())\n",
    "    \n",
    "    # Create dictionary to store cumulative losses\n",
    "    cumulative_losses = {crop: [] for crop in crops}\n",
    "    \n",
    "    # Calculate cumulative losses for each crop\n",
    "    for crop in crops:\n",
    "        cum_loss = 0\n",
    "        crop_data = df[df['crop_type'] == crop].sort_values('loss_percentage', ascending=False)\n",
    "        \n",
    "        for stage in stages:\n",
    "            stage_loss = crop_data[crop_data['stage'] == stage]['loss_percentage'].values\n",
    "            if len(stage_loss) > 0:\n",
    "                cum_loss += stage_loss[0]\n",
    "            \n",
    "            cumulative_losses[crop].append(cum_loss)\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for crop in crops:\n",
    "        plt.plot(stages, cumulative_losses[crop], marker='o', linewidth=2, label=crop)\n",
    "    \n",
    "    plt.title('Cumulative Post-Harvest Losses Across Value Chain', fontsize=16)\n",
    "    plt.xlabel('Value Chain Stage', fontsize=14)\n",
    "    plt.ylabel('Cumulative Loss (%)', fontsize=14)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(title='Crop Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/cumulative_value_chain_losses.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 3. Create a heatmap of losses by stage and crop type\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = df.pivot_table(\n",
    "        values='loss_percentage',\n",
    "        index='stage',\n",
    "        columns='crop_type',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=0.5)\n",
    "    plt.title('Post-Harvest Losses by Value Chain Stage and Crop Type (%)', fontsize=16)\n",
    "    plt.xlabel('Crop Type', fontsize=14)\n",
    "    plt.ylabel('Value Chain Stage', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/value_chain_heatmap.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    print(\"  - Value chain visualizations created successfully\")\n",
    "\n",
    "# Function to create financial impact visualizations\n",
    "def visualize_financial_impact(file_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for financial impact data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating financial impact visualizations...\")\n",
    "    \n",
    "    if file_path is None:\n",
    "        file_path = config['datasets'].get('financial_impact')\n",
    "    \n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"  - Financial impact data file not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded data shape: {df.shape}\")\n",
    "    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'crop_type' not in df.columns or 'financial_value' not in df.columns:\n",
    "        print(\"  - Error: Data does not have the expected columns (crop_type, financial_value)\")\n",
    "        return\n",
    "    \n",
    "    # 1. Create a bar chart of financial losses by crop type\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Sort by financial value\n",
    "    sorted_df = df.sort_values('financial_value', ascending=False)\n",
    "    \n",
    "    # Create the plot\n",
    "    ax = sns.barplot(x='crop_type', y='financial_value', data=sorted_df)\n",
    "    plt.title('Financial Impact of Post-Harvest Losses by Crop Type', fontsize=16)\n",
    "    plt.xlabel('Crop Type', fontsize=14)\n",
    "    plt.ylabel('Financial Loss (USD)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Format y-axis with million/billion suffixes\n",
    "    def millions(x, pos):\n",
    "        return format_large_number(x)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(millions))\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(sorted_df['financial_value']):\n",
    "        ax.text(i, v + v*0.02, format_large_number(v), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/financial_impact_by_crop.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 2. Create a pie chart of financial losses by crop type\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Calculate percentage of total loss by crop\n",
    "    total = sorted_df['financial_value'].sum()\n",
    "    crop_percentages = sorted_df['financial_value'] / total * 100\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.pie(\n",
    "        sorted_df['financial_value'],\n",
    "        labels=sorted_df['crop_type'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        shadow=True,\n",
    "        explode=[0.05] * len(sorted_df),\n",
    "        textprops={'fontsize': 12}\n",
    "    )\n",
    "    plt.title('Share of Financial Losses by Crop Type', fontsize=16)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/financial_impact_pie.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 3. If region data is available, create a bar chart by region\n",
    "    if 'region' in df.columns and df['region'].nunique() > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Calculate total financial loss by region\n",
    "        region_losses = df.groupby('region')['financial_value'].sum().sort_values(ascending=False)\n",
    "        \n",
    "        # Create the plot\n",
    "        ax = sns.barplot(x=region_losses.index, y=region_losses.values)\n",
    "        plt.title('Financial Impact of Post-Harvest Losses by Region', fontsize=16)\n",
    "        plt.xlabel('Region', fontsize=14)\n",
    "        plt.ylabel('Financial Loss (USD)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Format y-axis with million/billion suffixes\n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(millions))\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(region_losses.values):\n",
    "            ax.text(i, v + v*0.02, format_large_number(v), ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_file = 'results/plots/financial_impact_by_region.png'\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    print(\"  - Financial impact visualizations created successfully\")\n",
    "\n",
    "# Function to create nutrient losses visualizations\n",
    "def visualize_nutrient_losses(file_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for nutrient losses data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating nutrient losses visualizations...\")\n",
    "    \n",
    "    if file_path is None:\n",
    "        file_path = config['datasets'].get('nutrient_losses')\n",
    "    \n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"  - Nutrient losses data file not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded data shape: {df.shape}\")\n",
    "    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if data has the expected structure\n",
    "    if 'nutrient' not in df.columns or 'crop_type' not in df.columns or 'nutrient_loss' not in df.columns:\n",
    "        print(\"  - Error: Data does not have the expected columns (nutrient, crop_type, nutrient_loss)\")\n",
    "        return\n",
    "    \n",
    "    # 1. Create a grouped bar chart of nutrient losses by crop type\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Get the top 5 nutrients by total loss\n",
    "    top_nutrients = df.groupby('nutrient')['nutrient_loss'].sum().nlargest(5).index.tolist()\n",
    "    \n",
    "    # Filter data for top nutrients\n",
    "    filtered_df = df[df['nutrient'].isin(top_nutrients)]\n",
    "    \n",
    "    # Create log scale for better visualization if values vary widely\n",
    "    if filtered_df['nutrient_loss'].max() / filtered_df['nutrient_loss'].min() > 100:\n",
    "        plt.yscale('log')\n",
    "        print(\"  - Using log scale for nutrient losses due to wide value range\")\n",
    "    \n",
    "    # Create the plot\n",
    "    ax = sns.barplot(x='crop_type', y='nutrient_loss', hue='nutrient', data=filtered_df)\n",
    "    plt.title('Top 5 Nutrients Lost Due to Post-Harvest Losses by Crop Type', fontsize=16)\n",
    "    plt.xlabel('Crop Type', fontsize=14)\n",
    "    plt.ylabel('Nutrient Loss (log scale)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(title='Nutrient', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/nutrient_losses_by_crop.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 2. Create a heatmap of nutrient losses\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = df.pivot_table(\n",
    "        values='nutrient_loss',\n",
    "        index='nutrient',\n",
    "        columns='crop_type',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Normalize to make comparison easier across nutrients with different scales\n",
    "    norm_pivot = pivot.div(pivot.max(axis=1), axis=0)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(norm_pivot, annot=False, cmap='YlOrRd', linewidths=0.5)\n",
    "    plt.title('Relative Nutrient Losses by Crop Type (Normalized)', fontsize=16)\n",
    "    plt.xlabel('Crop Type', fontsize=14)\n",
    "    plt.ylabel('Nutrient', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = 'results/plots/nutrient_losses_heatmap.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    print(\"  - Nutrient losses visualizations created successfully\")\n",
    "\n",
    "# Function to create climate data visualizations\n",
    "def visualize_climate_data(file_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for climate data\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating climate data visualizations...\")\n",
    "    \n",
    "    if file_path is None:\n",
    "        file_path = config['datasets'].get('climate_data')\n",
    "    \n",
    "    if not file_path or not os.path.exists(file_path):\n",
    "        print(f\"  - Climate data file not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  - Loaded data shape: {df.shape}\")\n",
    "    print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if data has temperature or precipitation columns\n",
    "    temp_cols = [col for col in df.columns if 'temp' in col.lower()]\n",
    "    precip_cols = [col for col in df.columns if 'precip' in col.lower() or 'rain' in col.lower()]\n",
    "    \n",
    "    if not temp_cols and not precip_cols:\n",
    "        print(\"  - Error: Data does not have temperature or precipitation columns\")\n",
    "        return\n",
    "    \n",
    "    # Look for a category/month column\n",
    "    month_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['category', 'month', 'time', 'period']:\n",
    "            month_col = col\n",
    "            break\n",
    "    \n",
    "    if month_col is None:\n",
    "        # Use the first column as the category/month column\n",
    "        month_col = df.columns[0]\n",
    "    \n",
    "    # 1. Create a line chart with temperature data\n",
    "    if temp_cols:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create the plot with all temperature columns\n",
    "        for col in temp_cols:\n",
    "            plt.plot(df[month_col], df[col], marker='o', linewidth=2, label=col)\n",
    "        \n",
    "        plt.title('Temperature Trends in Nigeria', fontsize=16)\n",
    "        plt.xlabel(month_col, fontsize=14)\n",
    "        plt.ylabel('Temperature (Â°C)', fontsize=14)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend(title='Measurement', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Set reasonable y-axis limits for temperature\n",
    "        mean_temp = np.mean([df[col].mean() for col in temp_cols])\n",
    "        plt.ylim(mean_temp - 15, mean_temp + 15)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_file = 'results/plots/climate_temperature_trends.png'\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    # 2. Create a combined chart with temperature and precipitation\n",
    "    if temp_cols and precip_cols:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create primary axis for temperature\n",
    "        ax1 = plt.gca()\n",
    "        ax1.set_xlabel(month_col, fontsize=14)\n",
    "        ax1.set_ylabel('Temperature (Â°C)', fontsize=14, color='red')\n",
    "        ax1.tick_params(axis='y', colors='red')\n",
    "        \n",
    "        # Plot temperature (use the first temperature column)\n",
    "        temperature = df[temp_cols[0]]\n",
    "        ax1.plot(df[month_col], temperature, color='red', marker='o', linewidth=2)\n",
    "        \n",
    "        # Create secondary axis for precipitation\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Precipitation (mm)', fontsize=14, color='blue')\n",
    "        ax2.tick_params(axis='y', colors='blue')\n",
    "        \n",
    "        # Plot precipitation (use the first precipitation column)\n",
    "        precipitation = df[precip_cols[0]]\n",
    "        ax2.bar(df[month_col], precipitation, color='blue', alpha=0.6)\n",
    "        \n",
    "        plt.title('Temperature and Precipitation in Nigeria', fontsize=16)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Add custom legend\n",
    "        red_line = plt.Line2D([], [], color='red', marker='o', linestyle='-', linewidth=2)\n",
    "        blue_bar = plt.Rectangle((0,0), 1, 1, color='blue', alpha=0.6)\n",
    "        plt.legend([red_line, blue_bar], [temp_cols[0], precip_cols[0]], \n",
    "                  loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        output_file = 'results/plots/climate_temperature_precipitation.png'\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  - Saved plot to: {output_file}\")\n",
    "    \n",
    "    print(\"  - Climate data visualizations created successfully\")\n",
    "\n",
    "# Function to create integrated visualizations combining multiple datasets\n",
    "def create_integrated_visualizations():\n",
    "    \"\"\"\n",
    "    Create visualizations that integrate data from multiple datasets\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating integrated visualizations...\")\n",
    "    \n",
    "    # Load all available datasets\n",
    "    datasets = {}\n",
    "    \n",
    "    # Post-harvest losses\n",
    "    if 'post_harvest_losses' in config['datasets'] and os.path.exists(config['datasets']['post_harvest_losses']):\n",
    "        datasets['post_harvest_losses'] = pd.read_csv(config['datasets']['post_harvest_losses'])\n",
    "        print(f\"  - Loaded post-harvest losses data: {datasets['post_harvest_losses'].shape}\")\n",
    "    \n",
    "    # Value chain\n",
    "    if 'value_chain' in config['datasets'] and os.path.exists(config['datasets']['value_chain']):\n",
    "        datasets['value_chain'] = pd.read_csv(config['datasets']['value_chain'])\n",
    "        print(f\"  - Loaded value chain data: {datasets['value_chain'].shape}\")\n",
    "    \n",
    "    # Financial impact\n",
    "    if 'financial_impact' in config['datasets'] and os.path.exists(config['datasets']['financial_impact']):\n",
    "        datasets['financial_impact'] = pd.read_csv(config['datasets']['financial_impact'])\n",
    "        print(f\"  - Loaded financial impact data: {datasets['financial_impact'].shape}\")\n",
    "    \n",
    "    # Check if we have enough datasets for integrated visualizations\n",
    "    if len(datasets) < 2:\n",
    "        print(\"  - Not enough datasets available for integrated visualizations\")\n",
    "        return\n",
    "    \n",
    "    # 1. Integrated visualization: Post-harvest losses vs. Financial impact\n",
    "    if 'post_harvest_losses' in datasets and 'financial_impact' in datasets:\n",
    "        print(\"  - Creating integrated visualization of losses vs. financial impact\")\n",
    "        \n",
    "        # Get common crop types\n",
    "        ph_crops = set(datasets['post_harvest_losses']['crop_type'].unique())\n",
    "        fin_crops = set(datasets['financial_impact']['crop_type'].unique())\n",
    "        common_crops = list(ph_crops.intersection(fin_crops))\n",
    "        \n",
    "        if common_crops:\n",
    "            # Filter data for common crops\n",
    "            ph_filtered = datasets['post_harvest_losses'][\n",
    "                datasets['post_harvest_losses']['crop_type'].isin(common_crops)\n",
    "            ]\n",
    "            fin_filtered = datasets['financial_impact'][\n",
    "                datasets['financial_impact']['crop_type'].isin(common_crops)\n",
    "            ]\n",
    "            \n",
    "            # Calculate average loss by crop\n",
    "            ph_avg = ph_filtered.groupby('crop_type')['loss_percentage'].mean().reset_index()\n",
    "            \n",
    "            # Merge datasets\n",
    "            merged = pd.merge(\n",
    "                ph_avg,\n",
    "                fin_filtered[['crop_type', 'financial_value']],\n",
    "                on='crop_type',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            if len(merged) > 0:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Create scatter plot\n",
    "                plt.scatter(\n",
    "                    merged['loss_percentage'],\n",
    "                    merged['financial_value'],\n",
    "                    s=100,\n",
    "                    alpha=0.7\n",
    "                )\n",
    "                \n",
    "                # Add crop type labels to each point\n",
    "                for i, row in merged.iterrows():\n",
    "                    plt.annotate(\n",
    "                        row['crop_type'],\n",
    "                        (row['loss_percentage'], row['financial_value']),\n",
    "                        fontsize=12,\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points'\n",
    "                    )\n",
    "                \n",
    "                plt.title('Relationship Between Post-Harvest Losses and Financial Impact', fontsize=16)\n",
    "                plt.xlabel('Loss Percentage (%)', fontsize=14)\n",
    "                plt.ylabel('Financial Loss (USD)', fontsize=14)\n",
    "                plt.grid(alpha=0.3)\n",
    "                \n",
    "                # Format y-axis with million/billion suffixes\n",
    "                def millions(x, pos):\n",
    "                    return format_large_number(x)\n",
    "                \n",
    "                plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(millions))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save the plot\n",
    "                output_file = 'results/plots/integrated_losses_vs_financial.png'\n",
    "                plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"  - Saved plot to: {output_file}\")\n",
    "            else:\n",
    "                print(\"  - No matching data after merging datasets\")\n",
    "        else:\n",
    "            print(\"  - No common crop types found between datasets\")\n",
    "    \n",
    "    # 2. Integrated visualization: Value chain losses and financial impact\n",
    "    if 'value_chain' in datasets and 'financial_impact' in datasets:\n",
    "        print(\"  - Creating integrated visualization of value chain and financial impact\")\n",
    "        \n",
    "        # Get common crop types\n",
    "        vc_crops = set(datasets['value_chain']['crop_type'].unique())\n",
    "        fin_crops = set(datasets['financial_impact']['crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37367e24-6afc-4fb9-9095-9eac15d49a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
